import os
import glob
import numpy as np
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.preprocessing import image as keras_image
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from sklearn.metrics import accuracy_score

# ================= AYARLAR ================= #
IMAGE_DIR = r"C:\Users\pc\OneDrive\Documents\OneDrive\MasaÃ¼stÃ¼\pawmatesGP\data\archive (1)\images"
CACHE_FILE = "feature_cache_split.npy"
TEST_SPLIT_COUNT = 40  # Son 40 resmi test iÃ§in ayÄ±r
TEST_FEATURE_SIZE = 64
# =========================================== #

def get_model():
    print("ğŸ“¦ MobileNetV2 Modeli YÃ¼kleniyor...")
    return MobileNetV2(weights="imagenet", include_top=False, pooling="avg")

def extract_features(model, img_path):
    try:
        img = keras_image.load_img(img_path, target_size=(224, 224))
        x = preprocess_input(np.expand_dims(keras_image.img_to_array(img), axis=0))
        return model.predict(x, verbose=0)[0][:TEST_FEATURE_SIZE]
    except Exception as e:
        # print(f"Hata ({os.path.basename(img_path)}): {e}")
        return np.zeros(TEST_FEATURE_SIZE)

def run_test():
    if not os.path.exists(IMAGE_DIR):
        print("âŒ KlasÃ¶r bulunamadÄ±!")
        return

    model = get_model()
    
    all_files = glob.glob(os.path.join(IMAGE_DIR, "*.jpg"))
    breed_groups = {}
    
    # 1. DosyalarÄ± Grupla
    print("ğŸ“‚ Dosyalar taranÄ±yor...")
    for path in all_files:
        breed = os.path.basename(path).rsplit("_", 1)[0]
        if breed not in breed_groups: breed_groups[breed] = []
        breed_groups[breed].append(path)

    train_data = {} # HafÄ±za
    test_files = [] # Soru kaÄŸÄ±tlarÄ±
    test_labels = [] # Cevap anahtarÄ±

    # 2. Train/Test AyrÄ±mÄ± Yap
    print(f"âœ‚ï¸  Train/Test AyrÄ±mÄ± YapÄ±lÄ±yor (Son {TEST_SPLIT_COUNT} resim test)...")
    
    for breed, files in breed_groups.items():
        files.sort() # SÄ±ralama Ã¶nemli
        
        if len(files) > TEST_SPLIT_COUNT:
            # Train: BaÅŸtakiler
            train_subset = files[:-TEST_SPLIT_COUNT]
            # Test: Sondaki 40
            test_subset = files[-TEST_SPLIT_COUNT:]
            
            # Train verilerini hafÄ±zaya iÅŸle
            for f in train_subset:
                feat = extract_features(model, f)
                train_data[f] = feat
            
            # Test verilerini listeye ekle
            for f in test_subset:
                test_files.append(f)
                test_labels.append(breed)
        else:
            print(f"âš ï¸ {breed}: Yeterli sayÄ± yok, test dÄ±ÅŸÄ± bÄ±rakÄ±ldÄ±.")

    print(f"ğŸ§  EÄŸitim Seti Boyutu: {len(train_data)}")
    print(f"ğŸ“ Test Seti Boyutu: {len(test_files)}")
    print("-" * 30)
    print("ğŸš€ Test BaÅŸlÄ±yor...")

    # 3. Test Ä°ÅŸlemi (HÄ±zlÄ± KÄ±yaslama)
    train_paths = list(train_data.keys())
    train_feats = np.array(list(train_data.values()))
    train_labels_list = [os.path.basename(p).rsplit("_", 1)[0] for p in train_paths]

    predictions = []
    correct = 0

    for i, test_path in enumerate(test_files):
        # Ã–zellik Ã§Ä±kar
        target_feat = extract_features(model, test_path)
        
        # En yakÄ±n komÅŸuyu bul (Nearest Neighbor)
        dists = np.linalg.norm(train_feats - target_feat, axis=1)
        nearest_idx = np.argmin(dists)
        pred_breed = train_labels_list[nearest_idx]
        
        predictions.append(pred_breed)
        
        if pred_breed == test_labels[i]:
            correct += 1
            
        if (i+1) % 50 == 0:
            print(f"   Ä°ÅŸlenen: {i+1}/{len(test_files)} (AnlÄ±k DoÄŸruluk: %{(correct/(i+1))*100:.1f})")

    # 4. SonuÃ§
    final_acc = (correct / len(test_files)) * 100
    print("-" * 30)
    print(f"ğŸ† TEST SONUCU: %{final_acc:.2f}")
    print(f"âœ… DoÄŸru: {correct}")
    print(f"âŒ YanlÄ±ÅŸ: {len(test_files) - correct}")
    print("-" * 30)

if __name__ == "__main__":
    run_test()